{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd03de9d290524ff883b8259b248a1b147c991642eb880a4d565323a9dbe6df527f",
   "display_name": "Python 3.8.5 64-bit ('AiPy': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "initialize\n",
      "device: cuda\n",
      "Loading weights:  weights/model-f6b98070.pt\n",
      "start processing\n",
      "  processing input\\2.jpg (1/1)\n",
      "torch.Size([1, 384, 384]) (480, 852)\n",
      "<class 'numpy.ndarray'>\n",
      "Using cache found in C:\\Users\\naman/.cache\\torch\\hub\\facebookresearch_WSL-Images_master\n",
      "C:\\Users\\naman\\Miniconda3\\envs\\AiPy\\lib\\site-packages\\torch\\nn\\modules\\loss.py:446: UserWarning: Using a target size (torch.Size([480, 852])) that is different to the input size (torch.Size([1, 384, 384])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Traceback (most recent call last):\n",
      "  File \"midas_inference.py\", line 124, in <module>\n",
      "    run(args.input_path, args.output_path)\n",
      "  File \"midas_inference.py\", line 91, in run\n",
      "    final_loss = loss(prediction, torch.from_numpy(same_image).to(\"cuda\"))\n",
      "  File \"C:\\Users\\naman\\Miniconda3\\envs\\AiPy\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\naman\\Miniconda3\\envs\\AiPy\\lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 446, in forward\n",
      "    return F.mse_loss(input, target, reduction=self.reduction)\n",
      "  File \"C:\\Users\\naman\\Miniconda3\\envs\\AiPy\\lib\\site-packages\\torch\\nn\\functional.py\", line 2659, in mse_loss\n",
      "    expanded_input, expanded_target = torch.broadcast_tensors(input, target)\n",
      "  File \"C:\\Users\\naman\\Miniconda3\\envs\\AiPy\\lib\\site-packages\\torch\\functional.py\", line 71, in broadcast_tensors\n",
      "    return _VF.broadcast_tensors(tensors)  # type: ignore\n",
      "RuntimeError: The size of tensor a (384) must match the size of tensor b (852) at non-singleton dimension 2\n"
     ]
    }
   ],
   "source": [
    "!python midas_inference.py --i \"input\" --o \"output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(480, 852)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "import cv2\n",
    "image_one = cv2.imread(\"output/2.png\", cv2.IMREAD_UNCHANGED)\n",
    "same_image = cv2.imread(\"output/2.png\")\n",
    "\n",
    "image_one.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "loss = nn.MSELoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import Compose\n",
    "from utils.transforms import Resize, NormalizeImage, PrepareForNet\n",
    "from datasets.load_image_and_depth import LoadImageDepthAndLabels\n",
    "\n",
    "image_transforms = Compose(\n",
    "        [\n",
    "            Resize(\n",
    "                384,\n",
    "                384,\n",
    "                resize_target=False,\n",
    "                keep_aspect_ratio=False,\n",
    "                ensure_multiple_of=32,\n",
    "                resize_method=\"upper_bound\",\n",
    "                image_interpolation_method=cv2.INTER_CUBIC,\n",
    "            ),\n",
    "            NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            PrepareForNet(),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (384,384) (3,) ",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9a860ab14e78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtransformed_image_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_transforms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"image\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mimage_one\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtransformed_image_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformed_image_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtransformed_image_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_transforms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"image\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msame_image\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtransformed_image_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformed_image_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\AiPy\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\naman\\Desktop\\The Capstone Project\\doepd\\utils\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, sample)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__mean\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__std\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (384,384) (3,) "
     ]
    }
   ],
   "source": [
    "transformed_image_1 = image_transforms({\"image\": image_one})[\"image\"]\n",
    "transformed_image_1 = torch.from_numpy(transformed_image_1).to(\"cpu\")\n",
    "\n",
    "transformed_image_2 = image_transforms({\"image\": same_image})[\"image\"]\n",
    "transformed_image_2 = torch.from_numpy(transformed_image_2).to(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "loss(transformed_image_1, transformed_image_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "device: cuda\n",
      "Loading weights:  weights/model-f6b98070.pt\n",
      "Using cache found in C:\\Users\\naman/.cache\\torch\\hub\\facebookresearch_WSL-Images_master\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"midas_train.py\", line 81, in <module>\n",
      "    train(batch_size = BATCH_SIZE, epochs = EPOCHS)\n",
      "  File \"midas_train.py\", line 49, in train\n",
      "    for batch_idx, (image, target_depth) in enumerate(pbar):\n",
      "  File \"C:\\Users\\naman\\Miniconda3\\envs\\AiPy\\lib\\site-packages\\tqdm\\std.py\", line 1165, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"C:\\Users\\naman\\Miniconda3\\envs\\AiPy\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 435, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"C:\\Users\\naman\\Miniconda3\\envs\\AiPy\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1085, in _next_data\n",
      "    return self._process_data(data)\n",
      "  File \"C:\\Users\\naman\\Miniconda3\\envs\\AiPy\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1111, in _process_data\n",
      "    data.reraise()\n",
      "  File \"C:\\Users\\naman\\Miniconda3\\envs\\AiPy\\lib\\site-packages\\torch\\_utils.py\", line 428, in reraise\n",
      "    raise self.exc_type(msg)\n",
      "ValueError: Caught ValueError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"C:\\Users\\naman\\Miniconda3\\envs\\AiPy\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 198, in _worker_loop\n",
      "    data = fetcher.fetch(index)\n",
      "  File \"C:\\Users\\naman\\Miniconda3\\envs\\AiPy\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 44, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"C:\\Users\\naman\\Miniconda3\\envs\\AiPy\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 44, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"c:\\Users\\naman\\Desktop\\The Capstone Project\\doepd\\datasets\\load_image_and_depth.py\", line 30, in __getitem__\n",
      "    return torch.from_numpy(image), torch.from_numpy(np.transpose(depth, (2, 0, 1)))\n",
      "  File \"<__array_function__ internals>\", line 5, in transpose\n",
      "  File \"C:\\Users\\naman\\Miniconda3\\envs\\AiPy\\lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 653, in transpose\n",
      "    return _wrapfunc(a, 'transpose', axes)\n",
      "  File \"C:\\Users\\naman\\Miniconda3\\envs\\AiPy\\lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 58, in _wrapfunc\n",
      "    return bound(*args, **kwds)\n",
      "ValueError: axes don't match array\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "!python midas_train.py"
   ]
  }
 ]
}