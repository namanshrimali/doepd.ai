{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd03de9d290524ff883b8259b248a1b147c991642eb880a4d565323a9dbe6df527f",
   "display_name": "Python 3.8.5 64-bit ('AiPy': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading weights:  weights/model-f6b98070.pt\n",
      "Using cache found in C:\\Users\\naman/.cache\\torch\\hub\\facebookresearch_WSL-Images_master\n",
      "Model Summary: 48 layers, 5.21710e+06 parameters, 5.21710e+06 gradients\n"
     ]
    }
   ],
   "source": [
    "from models.doepd_net import DoepdNet\n",
    "model = DoepdNet(train_mode='yolo').to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 192, 192]           9,408\n       BatchNorm2d-2         [-1, 64, 192, 192]             128\n              ReLU-3         [-1, 64, 192, 192]               0\n         MaxPool2d-4           [-1, 64, 96, 96]               0\n            Conv2d-5          [-1, 256, 96, 96]          16,384\n       BatchNorm2d-6          [-1, 256, 96, 96]             512\n              ReLU-7          [-1, 256, 96, 96]               0\n            Conv2d-8          [-1, 256, 96, 96]          18,432\n       BatchNorm2d-9          [-1, 256, 96, 96]             512\n             ReLU-10          [-1, 256, 96, 96]               0\n           Conv2d-11          [-1, 256, 96, 96]          65,536\n      BatchNorm2d-12          [-1, 256, 96, 96]             512\n           Conv2d-13          [-1, 256, 96, 96]          16,384\n      BatchNorm2d-14          [-1, 256, 96, 96]             512\n             ReLU-15          [-1, 256, 96, 96]               0\n       Bottleneck-16          [-1, 256, 96, 96]               0\n           Conv2d-17          [-1, 256, 96, 96]          65,536\n      BatchNorm2d-18          [-1, 256, 96, 96]             512\n             ReLU-19          [-1, 256, 96, 96]               0\n           Conv2d-20          [-1, 256, 96, 96]          18,432\n      BatchNorm2d-21          [-1, 256, 96, 96]             512\n             ReLU-22          [-1, 256, 96, 96]               0\n           Conv2d-23          [-1, 256, 96, 96]          65,536\n      BatchNorm2d-24          [-1, 256, 96, 96]             512\n             ReLU-25          [-1, 256, 96, 96]               0\n       Bottleneck-26          [-1, 256, 96, 96]               0\n           Conv2d-27          [-1, 256, 96, 96]          65,536\n      BatchNorm2d-28          [-1, 256, 96, 96]             512\n             ReLU-29          [-1, 256, 96, 96]               0\n           Conv2d-30          [-1, 256, 96, 96]          18,432\n      BatchNorm2d-31          [-1, 256, 96, 96]             512\n             ReLU-32          [-1, 256, 96, 96]               0\n           Conv2d-33          [-1, 256, 96, 96]          65,536\n      BatchNorm2d-34          [-1, 256, 96, 96]             512\n             ReLU-35          [-1, 256, 96, 96]               0\n       Bottleneck-36          [-1, 256, 96, 96]               0\n           Conv2d-37          [-1, 512, 96, 96]         131,072\n      BatchNorm2d-38          [-1, 512, 96, 96]           1,024\n             ReLU-39          [-1, 512, 96, 96]               0\n           Conv2d-40          [-1, 512, 48, 48]          73,728\n      BatchNorm2d-41          [-1, 512, 48, 48]           1,024\n             ReLU-42          [-1, 512, 48, 48]               0\n           Conv2d-43          [-1, 512, 48, 48]         262,144\n      BatchNorm2d-44          [-1, 512, 48, 48]           1,024\n           Conv2d-45          [-1, 512, 48, 48]         131,072\n      BatchNorm2d-46          [-1, 512, 48, 48]           1,024\n             ReLU-47          [-1, 512, 48, 48]               0\n       Bottleneck-48          [-1, 512, 48, 48]               0\n           Conv2d-49          [-1, 512, 48, 48]         262,144\n      BatchNorm2d-50          [-1, 512, 48, 48]           1,024\n             ReLU-51          [-1, 512, 48, 48]               0\n           Conv2d-52          [-1, 512, 48, 48]          73,728\n      BatchNorm2d-53          [-1, 512, 48, 48]           1,024\n             ReLU-54          [-1, 512, 48, 48]               0\n           Conv2d-55          [-1, 512, 48, 48]         262,144\n      BatchNorm2d-56          [-1, 512, 48, 48]           1,024\n             ReLU-57          [-1, 512, 48, 48]               0\n       Bottleneck-58          [-1, 512, 48, 48]               0\n           Conv2d-59          [-1, 512, 48, 48]         262,144\n      BatchNorm2d-60          [-1, 512, 48, 48]           1,024\n             ReLU-61          [-1, 512, 48, 48]               0\n           Conv2d-62          [-1, 512, 48, 48]          73,728\n      BatchNorm2d-63          [-1, 512, 48, 48]           1,024\n             ReLU-64          [-1, 512, 48, 48]               0\n           Conv2d-65          [-1, 512, 48, 48]         262,144\n      BatchNorm2d-66          [-1, 512, 48, 48]           1,024\n             ReLU-67          [-1, 512, 48, 48]               0\n       Bottleneck-68          [-1, 512, 48, 48]               0\n           Conv2d-69          [-1, 512, 48, 48]         262,144\n      BatchNorm2d-70          [-1, 512, 48, 48]           1,024\n             ReLU-71          [-1, 512, 48, 48]               0\n           Conv2d-72          [-1, 512, 48, 48]          73,728\n      BatchNorm2d-73          [-1, 512, 48, 48]           1,024\n             ReLU-74          [-1, 512, 48, 48]               0\n           Conv2d-75          [-1, 512, 48, 48]         262,144\n      BatchNorm2d-76          [-1, 512, 48, 48]           1,024\n             ReLU-77          [-1, 512, 48, 48]               0\n       Bottleneck-78          [-1, 512, 48, 48]               0\n           Conv2d-79         [-1, 1024, 48, 48]         524,288\n      BatchNorm2d-80         [-1, 1024, 48, 48]           2,048\n             ReLU-81         [-1, 1024, 48, 48]               0\n           Conv2d-82         [-1, 1024, 24, 24]         294,912\n      BatchNorm2d-83         [-1, 1024, 24, 24]           2,048\n             ReLU-84         [-1, 1024, 24, 24]               0\n           Conv2d-85         [-1, 1024, 24, 24]       1,048,576\n      BatchNorm2d-86         [-1, 1024, 24, 24]           2,048\n           Conv2d-87         [-1, 1024, 24, 24]         524,288\n      BatchNorm2d-88         [-1, 1024, 24, 24]           2,048\n             ReLU-89         [-1, 1024, 24, 24]               0\n       Bottleneck-90         [-1, 1024, 24, 24]               0\n           Conv2d-91         [-1, 1024, 24, 24]       1,048,576\n      BatchNorm2d-92         [-1, 1024, 24, 24]           2,048\n             ReLU-93         [-1, 1024, 24, 24]               0\n           Conv2d-94         [-1, 1024, 24, 24]         294,912\n      BatchNorm2d-95         [-1, 1024, 24, 24]           2,048\n             ReLU-96         [-1, 1024, 24, 24]               0\n           Conv2d-97         [-1, 1024, 24, 24]       1,048,576\n      BatchNorm2d-98         [-1, 1024, 24, 24]           2,048\n             ReLU-99         [-1, 1024, 24, 24]               0\n      Bottleneck-100         [-1, 1024, 24, 24]               0\n          Conv2d-101         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-102         [-1, 1024, 24, 24]           2,048\n            ReLU-103         [-1, 1024, 24, 24]               0\n          Conv2d-104         [-1, 1024, 24, 24]         294,912\n     BatchNorm2d-105         [-1, 1024, 24, 24]           2,048\n            ReLU-106         [-1, 1024, 24, 24]               0\n          Conv2d-107         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-108         [-1, 1024, 24, 24]           2,048\n            ReLU-109         [-1, 1024, 24, 24]               0\n      Bottleneck-110         [-1, 1024, 24, 24]               0\n          Conv2d-111         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-112         [-1, 1024, 24, 24]           2,048\n            ReLU-113         [-1, 1024, 24, 24]               0\n          Conv2d-114         [-1, 1024, 24, 24]         294,912\n     BatchNorm2d-115         [-1, 1024, 24, 24]           2,048\n            ReLU-116         [-1, 1024, 24, 24]               0\n          Conv2d-117         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-118         [-1, 1024, 24, 24]           2,048\n            ReLU-119         [-1, 1024, 24, 24]               0\n      Bottleneck-120         [-1, 1024, 24, 24]               0\n          Conv2d-121         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-122         [-1, 1024, 24, 24]           2,048\n            ReLU-123         [-1, 1024, 24, 24]               0\n          Conv2d-124         [-1, 1024, 24, 24]         294,912\n     BatchNorm2d-125         [-1, 1024, 24, 24]           2,048\n            ReLU-126         [-1, 1024, 24, 24]               0\n          Conv2d-127         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-128         [-1, 1024, 24, 24]           2,048\n            ReLU-129         [-1, 1024, 24, 24]               0\n      Bottleneck-130         [-1, 1024, 24, 24]               0\n          Conv2d-131         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-132         [-1, 1024, 24, 24]           2,048\n            ReLU-133         [-1, 1024, 24, 24]               0\n          Conv2d-134         [-1, 1024, 24, 24]         294,912\n     BatchNorm2d-135         [-1, 1024, 24, 24]           2,048\n            ReLU-136         [-1, 1024, 24, 24]               0\n          Conv2d-137         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-138         [-1, 1024, 24, 24]           2,048\n            ReLU-139         [-1, 1024, 24, 24]               0\n      Bottleneck-140         [-1, 1024, 24, 24]               0\n          Conv2d-141         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-142         [-1, 1024, 24, 24]           2,048\n            ReLU-143         [-1, 1024, 24, 24]               0\n          Conv2d-144         [-1, 1024, 24, 24]         294,912\n     BatchNorm2d-145         [-1, 1024, 24, 24]           2,048\n            ReLU-146         [-1, 1024, 24, 24]               0\n          Conv2d-147         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-148         [-1, 1024, 24, 24]           2,048\n            ReLU-149         [-1, 1024, 24, 24]               0\n      Bottleneck-150         [-1, 1024, 24, 24]               0\n          Conv2d-151         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-152         [-1, 1024, 24, 24]           2,048\n            ReLU-153         [-1, 1024, 24, 24]               0\n          Conv2d-154         [-1, 1024, 24, 24]         294,912\n     BatchNorm2d-155         [-1, 1024, 24, 24]           2,048\n            ReLU-156         [-1, 1024, 24, 24]               0\n          Conv2d-157         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-158         [-1, 1024, 24, 24]           2,048\n            ReLU-159         [-1, 1024, 24, 24]               0\n      Bottleneck-160         [-1, 1024, 24, 24]               0\n          Conv2d-161         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-162         [-1, 1024, 24, 24]           2,048\n            ReLU-163         [-1, 1024, 24, 24]               0\n          Conv2d-164         [-1, 1024, 24, 24]         294,912\n     BatchNorm2d-165         [-1, 1024, 24, 24]           2,048\n            ReLU-166         [-1, 1024, 24, 24]               0\n          Conv2d-167         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-168         [-1, 1024, 24, 24]           2,048\n            ReLU-169         [-1, 1024, 24, 24]               0\n      Bottleneck-170         [-1, 1024, 24, 24]               0\n          Conv2d-171         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-172         [-1, 1024, 24, 24]           2,048\n            ReLU-173         [-1, 1024, 24, 24]               0\n          Conv2d-174         [-1, 1024, 24, 24]         294,912\n     BatchNorm2d-175         [-1, 1024, 24, 24]           2,048\n            ReLU-176         [-1, 1024, 24, 24]               0\n          Conv2d-177         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-178         [-1, 1024, 24, 24]           2,048\n            ReLU-179         [-1, 1024, 24, 24]               0\n      Bottleneck-180         [-1, 1024, 24, 24]               0\n          Conv2d-181         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-182         [-1, 1024, 24, 24]           2,048\n            ReLU-183         [-1, 1024, 24, 24]               0\n          Conv2d-184         [-1, 1024, 24, 24]         294,912\n     BatchNorm2d-185         [-1, 1024, 24, 24]           2,048\n            ReLU-186         [-1, 1024, 24, 24]               0\n          Conv2d-187         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-188         [-1, 1024, 24, 24]           2,048\n            ReLU-189         [-1, 1024, 24, 24]               0\n      Bottleneck-190         [-1, 1024, 24, 24]               0\n          Conv2d-191         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-192         [-1, 1024, 24, 24]           2,048\n            ReLU-193         [-1, 1024, 24, 24]               0\n          Conv2d-194         [-1, 1024, 24, 24]         294,912\n     BatchNorm2d-195         [-1, 1024, 24, 24]           2,048\n            ReLU-196         [-1, 1024, 24, 24]               0\n          Conv2d-197         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-198         [-1, 1024, 24, 24]           2,048\n            ReLU-199         [-1, 1024, 24, 24]               0\n      Bottleneck-200         [-1, 1024, 24, 24]               0\n          Conv2d-201         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-202         [-1, 1024, 24, 24]           2,048\n            ReLU-203         [-1, 1024, 24, 24]               0\n          Conv2d-204         [-1, 1024, 24, 24]         294,912\n     BatchNorm2d-205         [-1, 1024, 24, 24]           2,048\n            ReLU-206         [-1, 1024, 24, 24]               0\n          Conv2d-207         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-208         [-1, 1024, 24, 24]           2,048\n            ReLU-209         [-1, 1024, 24, 24]               0\n      Bottleneck-210         [-1, 1024, 24, 24]               0\n          Conv2d-211         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-212         [-1, 1024, 24, 24]           2,048\n            ReLU-213         [-1, 1024, 24, 24]               0\n          Conv2d-214         [-1, 1024, 24, 24]         294,912\n     BatchNorm2d-215         [-1, 1024, 24, 24]           2,048\n            ReLU-216         [-1, 1024, 24, 24]               0\n          Conv2d-217         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-218         [-1, 1024, 24, 24]           2,048\n            ReLU-219         [-1, 1024, 24, 24]               0\n      Bottleneck-220         [-1, 1024, 24, 24]               0\n          Conv2d-221         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-222         [-1, 1024, 24, 24]           2,048\n            ReLU-223         [-1, 1024, 24, 24]               0\n          Conv2d-224         [-1, 1024, 24, 24]         294,912\n     BatchNorm2d-225         [-1, 1024, 24, 24]           2,048\n            ReLU-226         [-1, 1024, 24, 24]               0\n          Conv2d-227         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-228         [-1, 1024, 24, 24]           2,048\n            ReLU-229         [-1, 1024, 24, 24]               0\n      Bottleneck-230         [-1, 1024, 24, 24]               0\n          Conv2d-231         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-232         [-1, 1024, 24, 24]           2,048\n            ReLU-233         [-1, 1024, 24, 24]               0\n          Conv2d-234         [-1, 1024, 24, 24]         294,912\n     BatchNorm2d-235         [-1, 1024, 24, 24]           2,048\n            ReLU-236         [-1, 1024, 24, 24]               0\n          Conv2d-237         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-238         [-1, 1024, 24, 24]           2,048\n            ReLU-239         [-1, 1024, 24, 24]               0\n      Bottleneck-240         [-1, 1024, 24, 24]               0\n          Conv2d-241         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-242         [-1, 1024, 24, 24]           2,048\n            ReLU-243         [-1, 1024, 24, 24]               0\n          Conv2d-244         [-1, 1024, 24, 24]         294,912\n     BatchNorm2d-245         [-1, 1024, 24, 24]           2,048\n            ReLU-246         [-1, 1024, 24, 24]               0\n          Conv2d-247         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-248         [-1, 1024, 24, 24]           2,048\n            ReLU-249         [-1, 1024, 24, 24]               0\n      Bottleneck-250         [-1, 1024, 24, 24]               0\n          Conv2d-251         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-252         [-1, 1024, 24, 24]           2,048\n            ReLU-253         [-1, 1024, 24, 24]               0\n          Conv2d-254         [-1, 1024, 24, 24]         294,912\n     BatchNorm2d-255         [-1, 1024, 24, 24]           2,048\n            ReLU-256         [-1, 1024, 24, 24]               0\n          Conv2d-257         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-258         [-1, 1024, 24, 24]           2,048\n            ReLU-259         [-1, 1024, 24, 24]               0\n      Bottleneck-260         [-1, 1024, 24, 24]               0\n          Conv2d-261         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-262         [-1, 1024, 24, 24]           2,048\n            ReLU-263         [-1, 1024, 24, 24]               0\n          Conv2d-264         [-1, 1024, 24, 24]         294,912\n     BatchNorm2d-265         [-1, 1024, 24, 24]           2,048\n            ReLU-266         [-1, 1024, 24, 24]               0\n          Conv2d-267         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-268         [-1, 1024, 24, 24]           2,048\n            ReLU-269         [-1, 1024, 24, 24]               0\n      Bottleneck-270         [-1, 1024, 24, 24]               0\n          Conv2d-271         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-272         [-1, 1024, 24, 24]           2,048\n            ReLU-273         [-1, 1024, 24, 24]               0\n          Conv2d-274         [-1, 1024, 24, 24]         294,912\n     BatchNorm2d-275         [-1, 1024, 24, 24]           2,048\n            ReLU-276         [-1, 1024, 24, 24]               0\n          Conv2d-277         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-278         [-1, 1024, 24, 24]           2,048\n            ReLU-279         [-1, 1024, 24, 24]               0\n      Bottleneck-280         [-1, 1024, 24, 24]               0\n          Conv2d-281         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-282         [-1, 1024, 24, 24]           2,048\n            ReLU-283         [-1, 1024, 24, 24]               0\n          Conv2d-284         [-1, 1024, 24, 24]         294,912\n     BatchNorm2d-285         [-1, 1024, 24, 24]           2,048\n            ReLU-286         [-1, 1024, 24, 24]               0\n          Conv2d-287         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-288         [-1, 1024, 24, 24]           2,048\n            ReLU-289         [-1, 1024, 24, 24]               0\n      Bottleneck-290         [-1, 1024, 24, 24]               0\n          Conv2d-291         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-292         [-1, 1024, 24, 24]           2,048\n            ReLU-293         [-1, 1024, 24, 24]               0\n          Conv2d-294         [-1, 1024, 24, 24]         294,912\n     BatchNorm2d-295         [-1, 1024, 24, 24]           2,048\n            ReLU-296         [-1, 1024, 24, 24]               0\n          Conv2d-297         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-298         [-1, 1024, 24, 24]           2,048\n            ReLU-299         [-1, 1024, 24, 24]               0\n      Bottleneck-300         [-1, 1024, 24, 24]               0\n          Conv2d-301         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-302         [-1, 1024, 24, 24]           2,048\n            ReLU-303         [-1, 1024, 24, 24]               0\n          Conv2d-304         [-1, 1024, 24, 24]         294,912\n     BatchNorm2d-305         [-1, 1024, 24, 24]           2,048\n            ReLU-306         [-1, 1024, 24, 24]               0\n          Conv2d-307         [-1, 1024, 24, 24]       1,048,576\n     BatchNorm2d-308         [-1, 1024, 24, 24]           2,048\n            ReLU-309         [-1, 1024, 24, 24]               0\n      Bottleneck-310         [-1, 1024, 24, 24]               0\n          Conv2d-311         [-1, 2048, 24, 24]       2,097,152\n     BatchNorm2d-312         [-1, 2048, 24, 24]           4,096\n            ReLU-313         [-1, 2048, 24, 24]               0\n          Conv2d-314         [-1, 2048, 12, 12]       1,179,648\n     BatchNorm2d-315         [-1, 2048, 12, 12]           4,096\n            ReLU-316         [-1, 2048, 12, 12]               0\n          Conv2d-317         [-1, 2048, 12, 12]       4,194,304\n     BatchNorm2d-318         [-1, 2048, 12, 12]           4,096\n          Conv2d-319         [-1, 2048, 12, 12]       2,097,152\n     BatchNorm2d-320         [-1, 2048, 12, 12]           4,096\n            ReLU-321         [-1, 2048, 12, 12]               0\n      Bottleneck-322         [-1, 2048, 12, 12]               0\n          Conv2d-323         [-1, 2048, 12, 12]       4,194,304\n     BatchNorm2d-324         [-1, 2048, 12, 12]           4,096\n            ReLU-325         [-1, 2048, 12, 12]               0\n          Conv2d-326         [-1, 2048, 12, 12]       1,179,648\n     BatchNorm2d-327         [-1, 2048, 12, 12]           4,096\n            ReLU-328         [-1, 2048, 12, 12]               0\n          Conv2d-329         [-1, 2048, 12, 12]       4,194,304\n     BatchNorm2d-330         [-1, 2048, 12, 12]           4,096\n            ReLU-331         [-1, 2048, 12, 12]               0\n      Bottleneck-332         [-1, 2048, 12, 12]               0\n          Conv2d-333         [-1, 2048, 12, 12]       4,194,304\n     BatchNorm2d-334         [-1, 2048, 12, 12]           4,096\n            ReLU-335         [-1, 2048, 12, 12]               0\n          Conv2d-336         [-1, 2048, 12, 12]       1,179,648\n     BatchNorm2d-337         [-1, 2048, 12, 12]           4,096\n            ReLU-338         [-1, 2048, 12, 12]               0\n          Conv2d-339         [-1, 2048, 12, 12]       4,194,304\n     BatchNorm2d-340         [-1, 2048, 12, 12]           4,096\n            ReLU-341         [-1, 2048, 12, 12]               0\n      Bottleneck-342         [-1, 2048, 12, 12]               0\n          Conv2d-343          [-1, 256, 48, 48]         131,328\n          Conv2d-344          [-1, 512, 24, 24]         524,800\n          Conv2d-345          [-1, 512, 12, 12]       1,049,088\n          Conv2d-346         [-1, 1024, 12, 12]       2,098,176\n          Conv2d-347           [-1, 27, 12, 12]          27,675\n       YOLOLayer-348         [-1, 3, 12, 12, 9]               0\n   FeatureConcat-349          [-1, 512, 12, 12]               0\n          Conv2d-350          [-1, 256, 12, 12]         131,072\n     BatchNorm2d-351          [-1, 256, 12, 12]             512\n       LeakyReLU-352          [-1, 256, 12, 12]               0\n        Upsample-353          [-1, 256, 24, 24]               0\n   FeatureConcat-354          [-1, 768, 24, 24]               0\n          Conv2d-355          [-1, 256, 24, 24]         196,608\n     BatchNorm2d-356          [-1, 256, 24, 24]             512\n       LeakyReLU-357          [-1, 256, 24, 24]               0\n          Conv2d-358          [-1, 512, 24, 24]       1,179,648\n     BatchNorm2d-359          [-1, 512, 24, 24]           1,024\n       LeakyReLU-360          [-1, 512, 24, 24]               0\n          Conv2d-361          [-1, 256, 24, 24]         131,072\n     BatchNorm2d-362          [-1, 256, 24, 24]             512\n       LeakyReLU-363          [-1, 256, 24, 24]               0\n          Conv2d-364          [-1, 512, 24, 24]       1,179,648\n     BatchNorm2d-365          [-1, 512, 24, 24]           1,024\n       LeakyReLU-366          [-1, 512, 24, 24]               0\n          Conv2d-367          [-1, 256, 24, 24]         131,072\n     BatchNorm2d-368          [-1, 256, 24, 24]             512\n       LeakyReLU-369          [-1, 256, 24, 24]               0\n          Conv2d-370          [-1, 512, 24, 24]       1,179,648\n     BatchNorm2d-371          [-1, 512, 24, 24]           1,024\n       LeakyReLU-372          [-1, 512, 24, 24]               0\n          Conv2d-373           [-1, 27, 24, 24]          13,851\n       YOLOLayer-374         [-1, 3, 24, 24, 9]               0\n   FeatureConcat-375          [-1, 256, 24, 24]               0\n          Conv2d-376          [-1, 128, 24, 24]          32,768\n     BatchNorm2d-377          [-1, 128, 24, 24]             256\n       LeakyReLU-378          [-1, 128, 24, 24]               0\n        Upsample-379          [-1, 128, 48, 48]               0\n   FeatureConcat-380          [-1, 384, 48, 48]               0\n          Conv2d-381          [-1, 128, 48, 48]          49,152\n     BatchNorm2d-382          [-1, 128, 48, 48]             256\n       LeakyReLU-383          [-1, 128, 48, 48]               0\n          Conv2d-384          [-1, 256, 48, 48]         294,912\n     BatchNorm2d-385          [-1, 256, 48, 48]             512\n       LeakyReLU-386          [-1, 256, 48, 48]               0\n          Conv2d-387          [-1, 128, 48, 48]          32,768\n     BatchNorm2d-388          [-1, 128, 48, 48]             256\n       LeakyReLU-389          [-1, 128, 48, 48]               0\n          Conv2d-390          [-1, 256, 48, 48]         294,912\n     BatchNorm2d-391          [-1, 256, 48, 48]             512\n       LeakyReLU-392          [-1, 256, 48, 48]               0\n          Conv2d-393          [-1, 128, 48, 48]          32,768\n     BatchNorm2d-394          [-1, 128, 48, 48]             256\n       LeakyReLU-395          [-1, 128, 48, 48]               0\n          Conv2d-396          [-1, 256, 48, 48]         294,912\n     BatchNorm2d-397          [-1, 256, 48, 48]             512\n       LeakyReLU-398          [-1, 256, 48, 48]               0\n          Conv2d-399           [-1, 27, 48, 48]           6,939\n       YOLOLayer-400         [-1, 3, 48, 48, 9]               0\n================================================================\nTotal params: 95,762,833\nTrainable params: 9,020,497\nNon-trainable params: 86,742,336\n----------------------------------------------------------------\nInput size (MB): 1.69\nForward/backward pass size (MB): 2388.78\nParams size (MB): 365.31\nEstimated Total Size (MB): 2755.77\n----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# !pip install torchsummary\n",
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_size=(3, 384, 384), device=\"cpu\")"
   ]
  }
 ]
}